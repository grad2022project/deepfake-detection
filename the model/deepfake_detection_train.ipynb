{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSYjHP9JzDvP","executionInfo":{"status":"ok","timestamp":1658104044328,"user_tz":-120,"elapsed":22419,"user":{"displayName":"Graduation Project","userId":"09322295020423481474"}},"outputId":"9160b884-a637-427e-d94d-6fca61d63564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"izzx0V5BzCJ8"},"outputs":[],"source":["import os\n","import cv2\n","import json\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import pandas as pd\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4JI2NS7zCJ_","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1658104383223,"user_tz":-120,"elapsed":29389,"user":{"displayName":"Graduation Project","userId":"09322295020423481474"}},"outputId":"1b5f2ce9-ab03-4393-c88b-1a75cdf93cad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 6998 images belonging to 2 classes.\n","Found 3569 images belonging to 2 classes.\n"]},{"output_type":"execute_result","data":{"text/plain":["\"\\ndata_dir = '/content/drive/MyDrive/Captured frames/train(700)'\\n\\nreal_data = [f for f in os.listdir(data_dir+'/real') if f.endswith('.png')]\\nfake_data = [f for f in os.listdir(data_dir+'/fake') if f.endswith('.png')]\\n\\nX = []\\nY = []\\n\\nfor img in real_data:\\n    X.append(img_to_array(load_img(data_dir+'/real/'+img)).flatten() / 255.0)\\n    Y.append(1)\\nfor img in fake_data:\\n    X.append(img_to_array(load_img(data_dir+'/fake/'+img)).flatten() / 255.0)\\n    Y.append(0)\\n\\nY_val_org = Y\\n\\n#Normalization\\nX = np.array(X)\\nY = to_categorical(Y, 2)\\n\\n#Reshape\\nX = X.reshape(-1, 224, 224, 3)\\n\\n#Train-Test split\\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state=5)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["input_shape = (224, 224, 3)\n","\n","base_path = '/content/drive/MyDrive/Captured frames/'\n","\n","image_gen = ImageDataGenerator(rescale=1./255.)\n","train_flow = image_gen.flow_from_directory(\n","    base_path + 'train(1600)/',\n","    target_size=(224, 224),\n","    batch_size=64,\n","    class_mode='binary'\n",")\n","\n","image_gen1 = ImageDataGenerator(rescale=1./255.)\n","valid_flow = image_gen1.flow_from_directory(\n","    base_path + '400/',\n","    target_size=(224, 224),\n","    batch_size=64,\n","    class_mode='binary'\n",")\n","\n"]},{"cell_type":"code","source":["'''\n","print(len(fake_data))\n","print(len(real_data))\n","print(len(X_train))\n","print(len(X_val))\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFDGfMhPt4jU","executionInfo":{"status":"ok","timestamp":1657971497858,"user_tz":-120,"elapsed":18,"user":{"displayName":"Graduation Project","userId":"09322295020423481474"}},"outputId":"d065fe47-cf1d-4feb-9564-6572d69c91cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4163\n","2835\n","5598\n","1400\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toEeak1pzCKA"},"outputs":[],"source":["from tensorflow.keras.applications.efficientnet import EfficientNetB0\n","\n","from tensorflow.keras.applications import InceptionResNetV2\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import InputLayer\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import GlobalMaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import Model\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","\n","googleNet_model = EfficientNetB0(include_top=False,  weights='imagenet', input_shape=input_shape)\n","googleNet_model.trainable = True\n","model = Sequential()\n","\n","model.add(googleNet_model)\n","model.add(GlobalAveragePooling2D())\n","model.add(Dense(units = 2, activation = 'sigmoid'))\n","#model.add(Flatten())\n","\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n","              metrics=['accuracy'])\n","model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVpjZdf7zCKC"},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss',\n","                               min_delta=0,\n","                               patience=2,\n","                               verbose=0, mode='auto')\n","EPOCHS = 15\n","BATCH_SIZE = 100\n","history = model.fit(train_flow, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = valid_flow, verbose = 1)"]},{"cell_type":"code","source":["model.save('/content/drive/MyDrive/test/deepfake-detection-tensor.h5')"],"metadata":{"id":"sFYUoW-HlFrl"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUHsrU8UzCKD"},"outputs":[],"source":["f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 4))\n","t = f.suptitle('Pre-trained InceptionResNetV2 Transfer Learn with Fine-Tuning & Image Augmentation Performance ', fontsize=12)\n","f.subplots_adjust(top=0.85, wspace=0.3)\n","\n","epoch_list = list(range(1,EPOCHS+1))\n","ax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\n","ax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\n","ax1.set_xticks(np.arange(0, EPOCHS+1, 1))\n","ax1.set_ylabel('Accuracy Value')\n","ax1.set_xlabel('Epoch #')\n","ax1.set_title('Accuracy')\n","l1 = ax1.legend(loc=\"best\")\n","\n","ax2.plot(epoch_list, history.history['loss'], label='Train Loss')\n","ax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\n","ax2.set_xticks(np.arange(0, EPOCHS+1, 1))\n","ax2.set_ylabel('Loss Value')\n","ax2.set_xlabel('Epoch #')\n","ax2.set_title('Loss')\n","l2 = ax2.legend(loc=\"best\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WbDHR9kQzCKE"},"outputs":[],"source":["#Output confusion matrix\n","def print_confusion_matrix(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    print('True positive = ', cm[0][0])\n","    print('False positive = ', cm[0][1])\n","    print('False negative = ', cm[1][0])\n","    print('True negative = ', cm[1][1])\n","    print('\\n')\n","    df_cm = pd.DataFrame(cm, range(2), range(2))\n","    sn.set(font_scale=1.4) # for label size\n","    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n","    plt.ylabel('Actual label', size = 20)\n","    plt.xlabel('Predicted label', size = 20)\n","    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n","    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n","    plt.ylim([2, 0])\n","    plt.show()\n","    \n","print_confusion_matrix(Y_val_org, model.predict_classes(X))"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:root] *","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"deepfake_detection_train.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}